{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VisionTransformers.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMRYGAYHRgNXQVDE2kqNr1V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wileyw/DeepLearningDemos/blob/master/VisionTransformers/VisionTransformers_with_PyTorch_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYUvD56k2udQ"
      },
      "source": [
        "# Vision Transformer\n",
        "\n",
        "[Cats and Dogs Vision Transformer](https://github.com/lucidrains/vit-pytorch/blob/main/examples/cats_and_dogs.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuDOjl3q2gmR"
      },
      "source": [
        "!pip3 -q install vit_pytorch linformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOeCqPoJ3Vwy"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import glob\n",
        "from itertools import chain\n",
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from linformer import Linformer\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from vit_pytorch.efficient import ViT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRMK_JT13YGH"
      },
      "source": [
        "print(f\"Torch: {torch.__version__}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba1QgUn83kjd"
      },
      "source": [
        "# Training settings\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "lr = 3e-5\n",
        "gamma = 0.7\n",
        "seed = 42"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnW_KuCc3nlU"
      },
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8msezx33qCN"
      },
      "source": [
        "device = 'cuda'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPM9sYeX3rzH"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKgltqZM3uF1"
      },
      "source": [
        "os.makedirs('data', exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NdMNwba3voT"
      },
      "source": [
        "train_dir = 'data/train'\n",
        "test_dir = 'data/test'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ27vt7N3yya"
      },
      "source": [
        "with zipfile.ZipFile('train.zip') as train_zip:\n",
        "    train_zip.extractall('data')\n",
        "    \n",
        "with zipfile.ZipFile('test.zip') as test_zip:\n",
        "    test_zip.extractall('data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNC_4-D730kN"
      },
      "source": [
        "train_list = glob.glob(os.path.join(train_dir,'*.jpg'))\n",
        "test_list = glob.glob(os.path.join(test_dir, '*.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXSfegjS32lb"
      },
      "source": [
        "print(f\"Train Data: {len(train_list)}\")\n",
        "print(f\"Test Data: {len(test_list)}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtkbSjJk34Re"
      },
      "source": [
        "labels = [path.split('/')[-1].split('.')[0] for path in train_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rug2j1AT36AK"
      },
      "source": [
        "# Random Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX-8OJ7237fu"
      },
      "source": [
        "random_idx = np.random.randint(1, len(train_list), size=9)\n",
        "fig, axes = plt.subplots(3, 3, figsize=(16, 12))\n",
        "\n",
        "for idx, ax in enumerate(axes.ravel()):\n",
        "    img = Image.open(train_list[idx])\n",
        "    ax.set_title(labels[idx])\n",
        "    ax.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmSp28Ri4hL_"
      },
      "source": [
        "# Split Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep6i6w0F4w9G"
      },
      "source": [
        "train_list, valid_list = train_test_split(train_list, \n",
        "                                          test_size=0.2,\n",
        "                                          stratify=labels,\n",
        "                                          random_state=seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCcUuKHw4z2-"
      },
      "source": [
        "print(f\"Train Data: {len(train_list)}\")\n",
        "print(f\"Validation Data: {len(valid_list)}\")\n",
        "print(f\"Test Data: {len(test_list)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VERuct5O4-H4"
      },
      "source": [
        "# Image Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gPj4YKo49ib"
      },
      "source": [
        "train_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "test_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aEZS0J-5DGg"
      },
      "source": [
        "# Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReLLEEon5Ljc"
      },
      "source": [
        "class CatsDogsDataset(Dataset):\n",
        "    def __init__(self, file_list, transform=None):\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        self.filelength = len(self.file_list)\n",
        "        return self.filelength\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.file_list[idx]\n",
        "        img = Image.open(img_path)\n",
        "        img_transformed = self.transform(img)\n",
        "\n",
        "        label = img_path.split(\"/\")[-1].split(\".\")[0]\n",
        "        label = 1 if label == \"dog\" else 0\n",
        "\n",
        "        return img_transformed, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfxhcsQv5OUZ"
      },
      "source": [
        "train_data = CatsDogsDataset(train_list, transform=train_transforms)\n",
        "valid_data = CatsDogsDataset(valid_list, transform=test_transforms)\n",
        "test_data = CatsDogsDataset(test_list, transform=test_transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag1cDhEK5Q3M"
      },
      "source": [
        "train_loader = DataLoader(dataset = train_data, batch_size=batch_size, shuffle=True )\n",
        "valid_loader = DataLoader(dataset = valid_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset = test_data, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMFVFgQ65VPP"
      },
      "source": [
        "print(len(train_data), len(train_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83fYc1w35XxO"
      },
      "source": [
        "print(len(valid_data), len(valid_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vLhgQlz5azv"
      },
      "source": [
        "# Efficient Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THV2ZaimL8HQ"
      },
      "source": [
        "# Vit Code\n",
        "import torch\n",
        "from torch import nn\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "class ViT2(nn.Module):\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, transformer, pool = 'cls', channels = 3):\n",
        "        super().__init__()\n",
        "        assert image_size % patch_size == 0, 'image dimensions must be divisible by the patch size'\n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        patch_dim = channels * patch_size ** 2\n",
        "\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),\n",
        "            nn.Linear(patch_dim, dim),\n",
        "        )\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.transformer = transformer\n",
        "\n",
        "        self.pool = pool\n",
        "        self.to_latent = nn.Identity()\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes)\n",
        "        )\n",
        "\n",
        "        # Testing\n",
        "        self.rearrange1 = Rearrange('b s d -> s b d')\n",
        "        self.rearrange2 = Rearrange('s b d -> b s d')\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.to_patch_embedding(img)\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "        x = self.rearrange1(x)\n",
        "        x = self.transformer(x)\n",
        "        x = self.rearrange2(x)\n",
        "\n",
        "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
        "\n",
        "        x = self.to_latent(x)\n",
        "        return self.mlp_head(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7_pTV1y5jcz"
      },
      "source": [
        "# Linformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJF5TgPEoTf3"
      },
      "source": [
        "- The cls-token is a summarization patch that takes input from all the patches\n",
        "- As an analogy, the cls-token encoded a zoomed out info. In contrast, the other patches encode the zoomed in patches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ2i9YUs5nqS"
      },
      "source": [
        "# TODO: Replace with PyTorch Transformer Encoder\n",
        "# Function is here: https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html\n",
        "efficient_transformer = Linformer(\n",
        "    dim=128,\n",
        "    seq_len=49+1,  # 7x7 patches + 1 cls-token\n",
        "    depth=12,\n",
        "    heads=8,\n",
        "    k=64\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qdr8wk_-IRzi"
      },
      "source": [
        "encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=8)\n",
        "efficient_transformer = transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=12).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "256-WN1i5qCG"
      },
      "source": [
        "# Visual Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyqqI4L_5uSS"
      },
      "source": [
        "# TODO: 1. Rewrite self.mlp_head = nn.Sequential()\n",
        "# TODO: 2. Remove einops\n",
        "# TODO: 3. Remove nn.Parameter\n",
        "# TODO: 4. Remove nn.Identity\n",
        "model = ViT2(\n",
        "    dim=128,\n",
        "    image_size=224,\n",
        "    patch_size=32,\n",
        "    num_classes=2,\n",
        "    transformer=efficient_transformer,\n",
        "    channels=3,\n",
        ").to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D76AlQu_5v3C"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHZ77iWh50Dq"
      },
      "source": [
        "# loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "# scheduler\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJtl67vu54W9"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "\n",
        "    for data, label in tqdm(train_loader):\n",
        "        data = data.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        output = model(data)\n",
        "        loss = criterion(output, label)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        acc = (output.argmax(dim=1) == label).float().mean()\n",
        "        epoch_accuracy += acc / len(train_loader)\n",
        "        epoch_loss += loss / len(train_loader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        epoch_val_accuracy = 0\n",
        "        epoch_val_loss = 0\n",
        "        for data, label in valid_loader:\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            val_output = model(data)\n",
        "            val_loss = criterion(val_output, label)\n",
        "\n",
        "            acc = (val_output.argmax(dim=1) == label).float().mean()\n",
        "            epoch_val_accuracy += acc / len(valid_loader)\n",
        "            epoch_val_loss += val_loss / len(valid_loader)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy:.4f}\\n\"\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}